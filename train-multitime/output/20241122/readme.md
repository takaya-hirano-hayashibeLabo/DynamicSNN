# 20241122
weight decayで過学習を防ぐ  
ここで, weight decay+Adamは相性が良くないらしい  
そこで, Adamを改善したAdamWを使用する

## cmd
~~~bash
cpulimit -f -l 150 -- python train_multitime_gesture.py  --target {target} --device 7
~~~

## モデル一覧

### dynasnn_dr0.5_dt0.001
そもそもdtはそこまで小さくしなくても良いかもしれない  
(dtを小さくしていたのは、いい感じにタイムスケーリングできるようにするためだった
しかし、グラフの見せ方でそこはなんとかなりそうなので、そこまでタイムスケーリングを意識する必要はなくなった)  

そこで、0.0001から0.001にdtを大きくしてみる  
これによって、90%精度が出ていたときの状態に近づくはず

パラメタはこれらなら、10倍速になっても、50%記憶が残るようになる
- dt=0.001
- tau=0.03