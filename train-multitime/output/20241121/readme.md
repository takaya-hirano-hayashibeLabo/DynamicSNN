# 20241121
以下の項目を試してみる
- dtめっちゃ小さく
- r(膜抵抗)を大きめに

## multilay-dynasnn
dtだけ小さくしたバージョン

## multilay-dynasnn_r50
膜抵抗を50倍にしたバージョン

## dynasnn_r50_dropout0.4_wd0.01
dt小さくしただけだと、trainには張り付くけど、testは精度が微妙になる  
そこで、過学習を抑えるために、dropoutとL2正則化を入れてみる  
dropoutを0.4にして, weight decayを0.01にしたバージョン

### ポイント
L2正則化は、単位円上にパラメータが分布するようになる  
このとき、biasや時定数もL2正則化しちゃうとおかしなことになる  
そこで、weight decayをかけるパラメタとかけないパラメタに分ける必要が出てくる

ちなみに、_missedは、分けなかった時のモデル  
かなり序盤で、全然学習ができなくなってる 