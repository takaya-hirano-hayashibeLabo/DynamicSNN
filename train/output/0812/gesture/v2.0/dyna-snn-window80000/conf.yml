train:
  batch: 20
  epoch: 100
  iter: -1 #ここは-1にするとmaxまで. とりあえず20くらいでいい
  lr: 0.001
  save_interval: 10 #epochごと
  datatype: gesture
  time-window: 80000 #μs (=80ms =0.08s)
  pool-size: -1 #transformのpooling size. 0以下にするとpoolingがなしになる
  pool-type: avg #transformのpooling type
  schedule-step: 20 #-1でscheduleなし
  schedule-rate: 0.5

model:
  type: dynamic-snn
  cnn-type: res #resで残差ブロックCNNになる

  in-size: 128
  in-channel: 2 
  out-size: 11
  hiddens: [8,12,24,32] 
  pool-type: avg
  pool-size: [2,2,2,2]
  residual-block: [2,2,2,2] #2にするとresidual blockあたり1+2層のCNNが積まれる
  is-bn: false #batch normalizationするかどうか. 基本なし. ラプラス変換から, あるとテスト時に上手くいかない

  linear-hidden: 512

  dropout: 0.5

  dt: 0.08 #s
  init-tau: 0.12 #dt/tauが1を超えないようにする.
  min-tau: 0.08
  v-threshold: 0.1
  v-rest: 0.0
  reset-mechanism: zero
  spike-grad: fast-sigmoid
  output-membrane: false